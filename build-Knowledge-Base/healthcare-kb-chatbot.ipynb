{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết\n",
    "!pip install requests beautifulsoup4 pandas numpy torch\n",
    "!pip install -U sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import các thư viện\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Tất cả thư viện đã được import thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Xây dựng bộ crawler dữ liệu từ Vinmec\n",
    "\n",
    "def get_article_links(category_url):\n",
    "    \"\"\"Lấy tất cả link bài viết từ một trang danh mục của Vinmec.\"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        response = requests.get(category_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Tìm tất cả các thẻ 'a' có href chứa '/tin-tuc/' trong phần body của trang\n",
    "            # Selector này cần được kiểm tra và cập nhật nếu cấu trúc web thay đổi\n",
    "            articles = soup.select('div.body a[href*=\"/tin-tuc/\"]')\n",
    "            for article in articles:\n",
    "                link = article.get('href')\n",
    "                if link and not link.startswith('http'):\n",
    "                    link = 'https://www.vinmec.com' + link\n",
    "                if link not in links:\n",
    "                    links.append(link)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi khi truy cập {category_url}: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_article_content(article_url):\n",
    "    \"\"\"Crawl tiêu đề và nội dung chi tiết của một bài viết.\"\"\"\n",
    "    data = {'url': article_url, 'title': '', 'content': ''}\n",
    "    try:\n",
    "        response = requests.get(article_url, timeout=10)\n",
    "        time.sleep(1) # Thêm độ trễ để tránh bị block\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Lấy tiêu đề bài viết\n",
    "            title_tag = soup.find('h1')\n",
    "            if title_tag:\n",
    "                data['title'] = title_tag.get_text(strip=True)\n",
    "            \n",
    "            # Lấy nội dung chính của bài viết\n",
    "            # Selector này thường chứa nội dung chính\n",
    "            content_div = soup.find('div', class_='post_content')\n",
    "            if content_div:\n",
    "                # Loại bỏ các tag không cần thiết như script, style\n",
    "                for tag in content_div(['script', 'style']):\n",
    "                    tag.decompose()\n",
    "                \n",
    "                raw_text = content_div.get_text(separator='\\n', strip=True)\n",
    "                # Dọn dẹp text\n",
    "                clean_text = re.sub(r'\\n{2,}', '\\n', raw_text) # Thay thế nhiều dòng trống bằng một\n",
    "                data['content'] = clean_text\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi khi crawl bài viết {article_url}: {e}\")\n",
    "        \n",
    "    return data\n",
    "\n",
    "# === Thực thi crawling ===\n",
    "# Ví dụ: Lấy các bài viết về bệnh Tiêu hóa\n",
    "CATEGORY_URL = \"https://www.vinmec.com/vi/tin-tuc/thong-tin-suc-khoe/tieu-hoa-gan-mat/\"\n",
    "print(f\"Bắt đầu crawl các link bài viết từ: {CATEGORY_URL}\")\n",
    "article_links = get_article_links(CATEGORY_URL)\n",
    "print(f\"Tìm thấy {len(article_links)} link bài viết.\")\n",
    "\n",
    "# Giới hạn số lượng bài viết để chạy demo nhanh\n",
    "article_links = article_links[:20] \n",
    "\n",
    "all_articles_data = []\n",
    "print(\"\\nBắt đầu crawl nội dung chi tiết của từng bài viết...\")\n",
    "for link in tqdm(article_links):\n",
    "    article_data = scrape_article_content(link)\n",
    "    if article_data and article_data['title'] and article_data['content']:\n",
    "        all_articles_data.append(article_data)\n",
    "\n",
    "print(f\"\\nHoàn thành! Crawl được {len(all_articles_data)} bài viết có nội dung.\")\n",
    "\n",
    "# Chuyển thành DataFrame để dễ xử lý\n",
    "df_health = pd.DataFrame(all_articles_data)\n",
    "df_health.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tiền xử lý và phân đoạn văn bản (Text Chunking)\n",
    "\n",
    "def split_text_into_chunks(text, max_chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Phân đoạn văn bản thành các chunks nhỏ hơn để vector hóa hiệu quả hơn.\n",
    "    Mỗi chunk sẽ là một đơn vị kiến thức.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "        \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(current_chunk) >= max_chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = current_chunk[-overlap:] # Giữ lại một phần overlap\n",
    "            \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# Tạo ra các đơn vị kiến thức (knowledge units)\n",
    "knowledge_units = []\n",
    "for index, row in df_health.iterrows():\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "    url = row['url']\n",
    "    \n",
    "    # Tạo một đoạn giới thiệu ban đầu\n",
    "    intro_chunk = f\"Tiêu đề: {title}. Nội dung: \" + \" \".join(content.split()[:100])\n",
    "    knowledge_units.append({\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'chunk': intro_chunk\n",
    "    })\n",
    "    \n",
    "    # Phân chia nội dung chính\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    for chunk in chunks:\n",
    "        knowledge_units.append({\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'chunk': chunk\n",
    "        })\n",
    "\n",
    "print(f\"Tổng số đơn vị kiến thức (chunks) được tạo: {len(knowledge_units)}\")\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_kb_health = pd.DataFrame(knowledge_units)\n",
    "df_kb_health.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vector hóa và xây dựng chỉ mục tìm kiếm với FAISS\n",
    "\n",
    "# Sử dụng một model đã được pre-trained cho tiếng Việt\n",
    "# 'bkai-foundation-models/vietnamese-bi-encoder' là một lựa chọn mạnh mẽ\n",
    "print(\"Bắt đầu tải model Sentence Transformer...\")\n",
    "model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "print(\"Model đã tải xong.\")\n",
    "\n",
    "# Lấy danh sách các chunk text để vector hóa\n",
    "chunks_to_encode = df_kb_health['chunk'].tolist()\n",
    "\n",
    "# Bắt đầu quá trình encoding\n",
    "print(f\"Bắt đầu vector hóa {len(chunks_to_encode)} chunks văn bản...\")\n",
    "# show_progress_bar=True để theo dõi tiến trình\n",
    "embeddings = model.encode(chunks_to_encode, show_progress_bar=True, normalize_embeddings=True)\n",
    "print(\"Vector hóa hoàn tất!\")\n",
    "print(f\"Shape của ma trận embeddings: {embeddings.shape}\") # (số chunks, chiều vector)\n",
    "\n",
    "# Xây dựng chỉ mục FAISS để tìm kiếm nhanh\n",
    "dimension = embeddings.shape[1]  # Chiều của vector\n",
    "index = faiss.IndexFlatL2(dimension) # Sử dụng L2 distance\n",
    "index = faiss.IndexIDMap(index) # Map id của vector với vị trí của nó trong dataframe\n",
    "\n",
    "# Thêm vectors vào chỉ mục\n",
    "ids = np.array(range(len(chunks_to_encode))).astype('int64')\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "print(f\"Đã thêm {index.ntotal} vector vào chỉ mục FAISS.\")\n",
    "\n",
    "# Lưu trữ Knowledge Base và chỉ mục FAISS\n",
    "# 1. Lưu DataFrame chứa thông tin gốc\n",
    "df_kb_health.to_csv('healthcare_kb.csv', index=False)\n",
    "\n",
    "# 2. Lưu chỉ mục FAISS\n",
    "with open('healthcare_faiss.index', 'wb') as f:\n",
    "    faiss.write_index(index, faiss.StandardIOWriter(f))\n",
    "\n",
    "print(\"Knowledge Base và chỉ mục FAISS đã được lưu thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Demo tìm kiếm thông tin trong Knowledge Base\n",
    "\n",
    "# Load lại KB và chỉ mục (giả sử ở một phiên làm việc khác)\n",
    "df_kb_loaded = pd.read_csv('healthcare_kb.csv')\n",
    "with open('healthcare_faiss.index', 'rb') as f:\n",
    "    index_loaded = faiss.read_index(faiss.StandardIOWriter(f))\n",
    "    \n",
    "model_loaded = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "\n",
    "\n",
    "def search_health_info(query, top_k=3):\n",
    "    \"\"\"Hàm tìm kiếm thông tin y tế trong KB.\"\"\"\n",
    "    print(f\"Truy vấn của bạn: '{query}'\")\n",
    "    \n",
    "    # 1. Vector hóa câu truy vấn\n",
    "    query_embedding = model_loaded.encode([query], normalize_embeddings=True)\n",
    "    \n",
    "    # 2. Tìm kiếm trong FAISS\n",
    "    distances, ids = index_loaded.search(query_embedding, top_k)\n",
    "    \n",
    "    # 3. Lấy kết quả\n",
    "    results = []\n",
    "    print(\"\\n--- Kết quả tìm kiếm phù hợp nhất ---\")\n",
    "    for i, doc_id in enumerate(ids[0]):\n",
    "        if doc_id != -1: # FAISS trả về -1 nếu không có đủ kết quả\n",
    "            result = {\n",
    "                \"score\": 1 - distances[0][i], # Chuyển L2 distance sang score (càng gần 1 càng tốt)\n",
    "                \"title\": df_kb_loaded.iloc[doc_id]['title'],\n",
    "                \"content_chunk\": df_kb_loaded.iloc[doc_id]['chunk'],\n",
    "                \"url\": df_kb_loaded.iloc[doc_id]['url']\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"\\nKết quả {i+1} (Score: {result['score']:.4f})\")\n",
    "            print(f\"   Tiêu đề: {result['title']}\")\n",
    "            print(f\"   Nguồn: {result['url']}\")\n",
    "            print(f\"   Nội dung liên quan: {result['content_chunk'][:500]}...\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Thử tìm kiếm\n",
    "search_health_info(\"triệu chứng của bệnh trào ngược dạ dày là gì?\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "search_health_info(\"cách chữa đau bụng do virus\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
