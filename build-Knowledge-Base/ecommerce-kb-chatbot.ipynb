{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết (nếu chưa có)\n",
    "!pip install requests beautifulsoup4 pandas numpy torch\n",
    "!pip install -U sentence-transformers faiss-cpu selenium\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "\n",
    "# Cấu hình cho Selenium trong Colab\n",
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import thư viện\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Khởi tạo driver cho Colab\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "print(\"Tất cả thư viện đã được import và Selenium đã được cấu hình!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Xây dựng Pipeline Tiền xử lý NLP Nâng cao\n",
    "\n",
    "# Danh sách stopword tiếng Việt được mở rộng\n",
    "VIETNAMESE_STOPWORDS = [\n",
    "    'a', 'anh', 'ba', 'bà', 'bác', 'bạn', 'bé', 'bên', 'bị', 'bỏ', 'bởi', 'ca', 'các', 'cách', 'cái',\n",
    "    'cần', 'càng', 'chắc', 'chẳng', 'chỉ', 'chiếc', 'cho', 'chú', 'chưa', 'chuyện', 'có', 'coi', 'cô',\n",
    "    'cũng', 'cùng', 'cứ', 'của', 'dạ', 'dành', 'do', 'đã', 'đang', 'đây', 'để', 'đến', 'đều',\n",
    "    'đi', 'điều', 'do', 'đó', 'được', 'em', 'gì', 'gồm', 'hay', 'hết', 'hiện', 'họ', 'hơn', 'khi',\n",
    "    'không', 'là', 'làm', 'lần', 'lên', 'lúc', 'mà', 'mình', 'mỗi', 'một', 'muốn', 'này', 'nên',\n",
    "    'nếu', 'ngay', 'ngoài', 'nhiều', 'như', 'nhưng', 'những', 'nơi', 'nữa', 'ông', 'qua', 'ra', 'rằng',\n",
    "    'rất', 'rồi', 'sau', 'sẽ', 'so', 'sự', 'tại', 'theo', 'thì', 'trên', 'trước', 'từ', 'từng', 'và',\n",
    "    'vẫn', 'vào', 'vậy', 'vì', 'việc', 'với', 'vừa', 'ạ', 'đấy', 'ấy', 'tôi', 'chúng tôi', 'chúng ta'\n",
    "]\n",
    "\n",
    "def preprocess_vietnamese_text_advanced(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Hàm tiền xử lý văn bản tiếng Việt nâng cao, tích hợp các kỹ thuật NLP thuần.\n",
    "    Pipeline: Chẩn hóa -> Tách từ & Gán nhãn từ loại -> Trích xuất cụm danh từ -> Loại bỏ stopword -> Tạo N-grams.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Chuẩn hóa Unicode và chuyển về chữ thường\n",
    "    text = unicodedata.normalize('NFC', text).lower()\n",
    "    \n",
    "    # 2. Xóa URL, email và các ký tự đặc biệt không cần thiết\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 3. Tách từ và Gán nhãn Từ loại (Part-of-Speech Tagging)\n",
    "    # Đây là bước nền tảng cho việc nhận diện cụm từ.\n",
    "    # Ví dụ: [('laptop', 'N'), ('gaming', 'N'), ('siêu', 'A'), ('mạnh', 'A')]\n",
    "    try:\n",
    "        pos_tags = pos_tag(text)\n",
    "    except Exception:\n",
    "        # Nếu pos_tag lỗi, dùng phương pháp cơ bản\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in tokens if word not in VIETNAMESE_STOPWORDS]\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    # 4. Trích xuất Cụm danh từ (Noun Phrase Chunking) và các từ quan trọng khác\n",
    "    # Ta sẽ giữ lại các Danh từ (N), Tính từ (A), và Động từ (V) vì chúng mang nhiều ngữ nghĩa nhất.\n",
    "    # Các cụm danh từ liền kề sẽ được ghép lại bằng dấu gạch dưới \"_\".\n",
    "    important_tokens = []\n",
    "    current_chunk = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Giữ lại các từ loại quan trọng: Noun, Proper Noun, Adjective, Verb\n",
    "        if tag.startswith('N') or tag.startswith('A') or tag.startswith('V'):\n",
    "            current_chunk.append(word)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                important_tokens.append(\"_\".join(current_chunk))\n",
    "                current_chunk = []\n",
    "    if current_chunk:\n",
    "        important_tokens.append(\"_\".join(current_chunk))\n",
    "\n",
    "    # 5. Loại bỏ Stopwords khỏi các token đã được xử lý\n",
    "    final_tokens = [token for token in important_tokens if token not in VIETNAMESE_STOPWORDS]\n",
    "    \n",
    "    # 6. (Tùy chọn) Tạo Bigrams từ các token còn lại để bắt ngữ cảnh\n",
    "    # Ví dụ: ['laptop_gaming', 'siêu_mạnh'] -> ['laptop_gaming', 'siêu_mạnh', 'laptop_gaming_siêu_mạnh']\n",
    "    bigrams = [\"_\".join(final_tokens[i:i+2]) for i in range(len(final_tokens)-1)]\n",
    "    \n",
    "    # Kết hợp token đơn và bigram để làm giàu thông tin\n",
    "    return \" \".join(final_tokens + bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"Laptop Gaming MSI Katana GF66 12UC (475VN) giá rất tốt, rất đáng để mua!!! Các bạn có nên mua không?\"\n",
    "cleaned_text_advanced = preprocess_vietnamese_text_advanced(sample_text)\n",
    "print(f\"Văn bản gốc: '{sample_text}'\")\n",
    "print(f\"Văn bản đã xử lý (Nâng cao): '{cleaned_text_advanced}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Xây dựng bộ crawler dữ liệu từ Tiki\n",
    "\n",
    "def get_product_links(category_url):\n",
    "    \"\"\"Lấy link của các sản phẩm từ trang danh mục Tiki.\"\"\"\n",
    "    driver.get(category_url)\n",
    "    time.sleep(5) # Chờ trang tải JavaScript\n",
    "    \n",
    "    # Lăn chuột xuống để tải thêm sản phẩm\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(2): # Lăn 2 lần\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    product_links = []\n",
    "    # Tiki sử dụng thẻ 'a' với attribute 'href' cho sản phẩm\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if '.html' in href and 'p' in href and 'spid' not in href:\n",
    "            if not href.startswith('https://tiki.vn'):\n",
    "                href = 'https://tiki.vn' + href\n",
    "            product_links.append(href)\n",
    "    return list(set(product_links)) # Loại bỏ link trùng lặp\n",
    "\n",
    "def scrape_product_details(product_url):\n",
    "    \"\"\"Crawl thông tin chi tiết của một sản phẩm.\"\"\"\n",
    "    try:\n",
    "        driver.get(product_url)\n",
    "        time.sleep(3) # Chờ tải\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Tên sản phẩm\n",
    "        name = soup.find('h1', class_='title').get_text(strip=True) if soup.find('h1', class_='title') else \"N/A\"\n",
    "\n",
    "        # Giá\n",
    "        price_str = soup.find('div', class_='product-price__current-price').get_text(strip=True) if soup.find('div', class_='product-price__current-price') else \"0\"\n",
    "        price = int(re.sub(r'[^\\d]', '', price_str))\n",
    "        \n",
    "        # Mô tả\n",
    "        description_div = soup.find('div', {'data-testid': 'desc-content'})\n",
    "        description = description_div.get_text(strip=True, separator='\\n') if description_div else \"\"\n",
    "\n",
    "        # Thông số kỹ thuật\n",
    "        specs = {}\n",
    "        spec_table = soup.find('table', class_='Content_table')\n",
    "        if spec_table:\n",
    "            rows = spec_table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:\n",
    "                    key = cells[0].get_text(strip=True)\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    specs[key] = value\n",
    "\n",
    "        return {\n",
    "            'url': product_url,\n",
    "            'name': name,\n",
    "            'price': price,\n",
    "            'description': description,\n",
    "            'specifications': specs\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi crawl sản phẩm {product_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Thực thi crawling ===\n",
    "CATEGORY_URL = \"https://tiki.vn/laptop/c1846\"\n",
    "print(f\"Bắt đầu crawl link sản phẩm từ: {CATEGORY_URL}\")\n",
    "product_links = get_product_links(CATEGORY_URL)\n",
    "print(f\"Tìm thấy {len(product_links)} link sản phẩm.\")\n",
    "\n",
    "# Giới hạn số lượng để demo\n",
    "product_links = product_links[:20]\n",
    "\n",
    "all_products_data = []\n",
    "print(\"\\nBắt đầu crawl thông tin chi tiết từng sản phẩm...\")\n",
    "for link in tqdm(product_links):\n",
    "    product_data = scrape_product_details(link)\n",
    "    if product_data and product_data['name'] != \"N/A\":\n",
    "        all_products_data.append(product_data)\n",
    "\n",
    "driver.quit() # Đóng trình duyệt\n",
    "print(f\"\\nHoàn thành! Crawl được {len(all_products_data)} sản phẩm.\")\n",
    "\n",
    "df_ecommerce = pd.DataFrame(all_products_data)\n",
    "df_ecommerce.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tạo trường văn bản tổng hợp để vector hóa\n",
    "\n",
    "def create_searchable_text(row):\n",
    "    \"\"\"Tạo một chuỗi văn bản duy nhất chứa các thông tin quan trọng nhất của sản phẩm.\"\"\"\n",
    "    name = row['name']\n",
    "    \n",
    "    # Chuyển đổi dict thông số kỹ thuật thành chuỗi\n",
    "    specs_list = []\n",
    "    if isinstance(row['specifications'], dict):\n",
    "        for key, value in row['specifications'].items():\n",
    "            specs_list.append(f\"{key}: {value}\")\n",
    "    specs_text = \". \".join(specs_list)\n",
    "    \n",
    "    # Ghép nối các thông tin\n",
    "    # Đây là bước quan trọng để tạo ngữ cảnh cho việc tìm kiếm\n",
    "    # Ví dụ: \"Laptop ABC, chip Intel Core i5, RAM 8GB. Đây là dòng laptop văn phòng...\"\n",
    "    searchable = f\"Tên sản phẩm: {name}. Thông số: {specs_text}. Mô tả: {row['description'][:300]}\"\n",
    "    return searchable\n",
    "\n",
    "df_ecommerce['searchable_text'] = df_ecommerce.apply(create_searchable_text, axis=1)\n",
    "\n",
    "# Lưu KB dạng thô\n",
    "df_ecommerce.to_json('ecommerce_kb.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "df_ecommerce[['name', 'searchable_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vector hóa và xây dựng chỉ mục tìm kiếm FAISS\n",
    "\n",
    "# Sử dụng lại model đã khai báo ở phần Healthcare\n",
    "# Nếu chưa có, hãy chạy lại cell tải model\n",
    "print(\"Bắt đầu tải model Sentence Transformer (nếu chưa có)...\")\n",
    "model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "print(\"Model đã sẵn sàng.\")\n",
    "\n",
    "chunks_to_encode = df_ecommerce['searchable_text'].tolist()\n",
    "\n",
    "print(f\"Bắt đầu vector hóa {len(chunks_to_encode)} sản phẩm...\")\n",
    "embeddings = model.encode(chunks_to_encode, show_progress_bar=True, normalize_embeddings=True)\n",
    "print(\"Vector hóa hoàn tất!\")\n",
    "\n",
    "# Xây dựng chỉ mục FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index = faiss.IndexIDMap(index)\n",
    "ids = np.array(df_ecommerce.index).astype('int64') # Dùng index của DataFrame làm ID\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "print(f\"Đã thêm {index.ntotal} vector vào chỉ mục FAISS.\")\n",
    "\n",
    "# Lưu trữ\n",
    "# 1. Dữ liệu gốc đã được lưu ở file ecommerce_kb.json\n",
    "# 2. Lưu chỉ mục FAISS\n",
    "with open('ecommerce_faiss.index', 'wb') as f:\n",
    "    faiss.write_index(index, faiss.StandardIOWriter(f))\n",
    "\n",
    "print(\"Chỉ mục FAISS cho sản phẩm đã được lưu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Demo tìm kiếm sản phẩm trong Knowledge Base\n",
    "\n",
    "# Load lại KB và chỉ mục\n",
    "df_kb_loaded = pd.read_json('ecommerce_kb.json', orient='records', lines=True)\n",
    "with open('ecommerce_faiss.index', 'rb') as f:\n",
    "    index_loaded = faiss.read_index(faiss.StandardIOWriter(f))\n",
    "    \n",
    "model_loaded = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "\n",
    "def search_products(query, top_k=3):\n",
    "    \"\"\"Hàm tìm kiếm sản phẩm trong KB.\"\"\"\n",
    "    print(f\"Truy vấn của bạn: '{query}'\")\n",
    "    \n",
    "    query_embedding = model_loaded.encode([query], normalize_embeddings=True)\n",
    "    distances, ids = index_loaded.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    print(\"\\n--- Các sản phẩm phù hợp nhất ---\")\n",
    "    for i, doc_id in enumerate(ids[0]):\n",
    "        if doc_id != -1:\n",
    "            product = df_kb_loaded.iloc[doc_id]\n",
    "            score = 1 - distances[0][i]\n",
    "            \n",
    "            print(f\"\\nKết quả {i+1} (Score: {score:.4f})\")\n",
    "            print(f\"   Tên: {product['name']}\")\n",
    "            print(f\"   Giá: {product['price']:,}đ\")\n",
    "            print(f\"   URL: {product['url']}\")\n",
    "            \n",
    "            # In ra một vài thông số chính\n",
    "            specs = product.get('specifications', {})\n",
    "            if isinstance(specs, dict):\n",
    "                cpu = specs.get('CPU', 'N/A')\n",
    "                ram = specs.get('RAM', 'N/A')\n",
    "                print(f\"   CPU: {cpu}, RAM: {ram}\")\n",
    "            \n",
    "            results.append(product.to_dict())\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Thử tìm kiếm\n",
    "search_products(\"laptop mỏng nhẹ cho sinh viên\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "search_products(\"máy tính gaming có card đồ họa rời\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
